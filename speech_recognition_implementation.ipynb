{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"speechrecognition.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Ll6iaWKsspzw"},"source":["import scipy as sp\n","import scipy.io.wavfile as wavfile\n","import os\n","import gmmhmm as hmm\n","import MFCC\n","import numpy as np\n","import re\n","import random\n","import pickle\n","from tqdm import tqdm \n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oezWLZbDspz0"},"source":["Speech recognition is a cool application of Hidden Markov Models when we allow the state space to be continuous rather than discrete - Continuous Density Hidden Markov Models. Here I use one type of these, the Gaussian Mixture Model Hidden Markov Model.  \n","\n","\n","The following function accepts a GMMHMM as well as\n","an integer n_sim, and which simulates the GMMHMM process, generating n_sim different observations."]},{"cell_type":"code","metadata":{"id":"eUqqedbKspz1"},"source":["def sample_gmmhmm(gmmhmm, n_sim):\n","    \"\"\"\n","    Simulate from a GMMHMM.\n","    \n","    Returns\n","    -------\n","    states : ndarray of shape (n_sim,)\n","        The sequence of states\n","    obs : ndarray of shape (n_sim, K)\n","        The generated observations (vectors of length K)\n","    \"\"\"\n","    A, weights, means, covars, pi = gmmhmm \n","    states, obs = np.zeros(n_sim), np.zeros((n_sim, len(weights[0]))) \n","         \n","    for i in range(n_sim): \n","        # choose initial state\n","        state = np.argmax(np.random.multinomial(1, pi))\n","        # randomly sample\n","        sample_component = np.argmax(np.random.multinomial(1, weights[state,:])) \n","        sample = np.random.multivariate_normal(means[state, sample_component, :], \n","                                               covars[state, sample_component, :, :])\n","        # update states and obs arrays   \n","        states[i], obs[i] = state, sample                                \n","                                               \n","    return states, obs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hljaMdM5spz2"},"source":["A = np.array([[.65, .35], [.15, .85]])\n","pi = np.array([.8, .2])\n","weights = np.array([[.7, .2, .1], [.1, .5, .4]])\n","means1 = np.array([[0., 17., -4.], [5., -12., -8.], [-16., 22., 2.]])\n","means2 = np.array([[-5., 3., 23.], [-12., -2., 14.], [15., -32., 0.]])\n","means = np.array([means1, means2])\n","covars1 = np.array([5*np.eye(3), 7*np.eye(3), np.eye(3)])\n","covars2 = np.array([10*np.eye(3), 3*np.eye(3), 4*np.eye(3)])\n","covars = np.array([covars1, covars2])\n","gmmhmm = [A, weights, means, covars, pi] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ts_DT7sspz2","outputId":"692f0424-947f-4780-8929-cc6f8d733414"},"source":["sample_gmmhmm(gmmhmm, n_sim = 4)"],"execution_count":null,"outputs":[{"data":{"text/plain":["(array([1., 0., 1., 0.]),\n"," array([[ -4.08227529,   0.79816729,  19.43895353],\n","        [  0.12786178,  18.32327316,  -3.72006605],\n","        [ 12.69725559, -30.29012604,  -1.98135426],\n","        [ -0.6455404 ,  12.21394581,  -5.93210441]]))"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"CAcFYENvspz4"},"source":["## Problem 2\n","\n","Samples.zip contains 31 recordings for each of the words/phrases mathematics, biology, political science, psychology, and statistics. These audio samples are 2 seconds in\n","duration, recorded at a rate of 44100 samples per second, with samples stored as 16-bit signed\n","integers in WAV format. \n","Load the recordings into Python using scipy.io.wavfile.read\n","\n","Extract the MFCCs from each sample using code from the file MFCC.py.\n","Store the MFCCs for each word in a separate list. You should have five lists, each containing\n","31 MFCC arrays, corresponding to each of the five words under consideration."]},{"cell_type":"code","metadata":{"id":"uBzQiOscspz4","outputId":"f50e5aa7-272a-402f-fc10-d2024e56a100"},"source":["# skip the repeats, keep the mels in mels dict \n","repeats = {\"Biology00.wav\", \"Mathematics00.wav\", \"PoliticalScience.wav\", \n","           \"Psychology00.wav\", \"Statistics00.wav\"}\n","mels = {'Biology': [], 'Mathematics': [], 'PoliticalScience': [], \n","        'Psychology': [], 'Statistics': []} \n"," \n","filepath = \"./Samples\"\n","\n","# loop over files \n","for doc in os.listdir(filepath): \n","    if doc not in repeats:\n","        temp = doc.split(\" \")\n","        try: \n","            # get the mel., append to appropriate list \n","            num, x = wavfile.read(filepath + \"/\" + doc)  \n","            mel = MFCC.extract(x, show = False)   \n","            mels[temp[0]].append(mel)  \n","        except: \n","            continue\n","\n","# unpack lists and make sure there are 30 arrays \n","bio, math, polysci, psych, stats = mels.values() \n","for l in [bio, math, polysci, psych, stats]: \n","    print(len(l), end = \" \")  "],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["30 30 30 30 30 "]}]},{"cell_type":"markdown","metadata":{"id":"mqfsQx0kspz5"},"source":["## Problem 3\n","\n","Partition each list of MFCCs into a training set of 21 samples, and a test set of\n","the remaining 10 samples.\n","Using the training sets, train a GMMHMM on each of the words from the previous problem\n","with at least 10 random restarts, keeping the best model for each word (the one with the highest\n","log-likelihood). This process may take several minutes. Since you will not want to run this\n","more than once, you will want to save the best model for each word to disk using the pickle\n","module so that you can use it later."]},{"cell_type":"code","metadata":{"id":"GaIb-wZ1spz6"},"source":["def initialize(n_states):\n","    transmat = np.ones((n_states,n_states))/float(n_states)\n","    for i in range(n_states):\n","        transmat[i, :] += sp.random.uniform(-1./n_states,1./n_states,n_states)\n","        transmat[i, :] /= sum(transmat[i, :])\n","    startprob = np.ones(n_states)/float(n_states) + sp.random.uniform(-1./n_states,1./n_states,n_states)\n","    startprob /= sum(startprob)\n","    return startprob, transmat "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WV9Cp89Hspz7","outputId":"97dd945b-b602-439e-a7cd-d7b41722649f"},"source":["words = mels.keys() \n","samples = [bio, math, polysci, psych, stats] \n","\n","# loop over each word and its samples \n","for word, word_samples in zip(words, samples): \n","    \n","    # get traina dn test data \n","    x_train, x_test = word_samples[: 20], word_samples[20: ] \n","    best = -np.inf  \n","    loop = tqdm(range(10)) \n","    \n","    for i in loop: \n","        \n","        # traineach model 10 times !! \n","        startprob, transmat = initialize(5)\n","        model = hmm.GMMHMM(n_components=5, n_mix=3, transmat=transmat, startprob=startprob, cvtype='diag')\n","        model.covars_prior = 0.01\n","        model.fit(x_train, init_params='mc', var=0.1)\n","        \n","        # track the best model for each word \n","        if model.logprob > best: \n","            \n","            best = model.logprob \n","            best_model = model \n","            \n","    # save the models \n","    pickle.dump(best_model, open(\"{}.p\".format(word), \"wb\"))  "],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [05:52<00:00, 35.30s/it]\n","100%|██████████| 10/10 [09:30<00:00, 57.07s/it]\n","100%|██████████| 10/10 [08:27<00:00, 50.73s/it]\n","100%|██████████| 10/10 [09:14<00:00, 55.44s/it]\n","100%|██████████| 10/10 [07:34<00:00, 45.48s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"rcoBrBKRspz7","outputId":"99de2297-98f0-4e20-e561-5fd9f8273f49"},"source":["best "],"execution_count":null,"outputs":[{"data":{"text/plain":["-30489.250566198258"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"fGH69wlyspz7"},"source":["## Problem 4\n","\n","Classify the 10 test samples for each word. Make a dictionary containing the accuracy\n","of the classification of your five testing sets where the words/phrases are the keys,\n","and the values are the percent accuracy.\n","\n","Write a few sentences answering the following questions:\n","How does your system perform?\n","Which words are the hardest to correctly classify?"]},{"cell_type":"code","metadata":{"id":"uVTo3Wdhspz8","outputId":"a232699a-3c8b-4931-fefc-75e8ba5e7f25"},"source":["# load in the models \n","models = [pickle.load(open(\"{}.p\".format(word), \"rb\")) for word in words] \n","\n","accs = {} \n","\n","# loop over the words and their samples \n","for index, (word, sample) in enumerate(zip(words, samples)): \n","    \n","    # get the test data and label \n","    x_test, y_test, preds = sample[20: ], index, [] \n","\n","    # get predicted label for each word \n","    for obs in x_test: \n","        y_hat = np.argmax([model.score(obs) for model in models]) \n","        preds.append(y_hat) \n","       \n","    # print results and update accuracy dictionary \n","    acc = 100 * np.mean([pred == y_test for pred in preds]) \n","    accs.update({word: acc})\n","    if word == \"Biology\": \n","        print(\"Accuracy for {}: \\t\\t{:.2f}%\".format(word, acc)) \n","    else: \n","        print(\"Accuracy for {}: \\t{:.2f}%\".format(word, acc)) "],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy for Biology: \t\t100.00%\n","Accuracy for Mathematics: \t100.00%\n","Accuracy for PoliticalScience: \t90.00%\n","Accuracy for Psychology: \t100.00%\n","Accuracy for Statistics: \t100.00%\n"]}]},{"cell_type":"code","metadata":{"id":"1SoHjegwspz8","outputId":"71c8e80f-f447-411f-e337-d83ac644d950"},"source":["accs"],"execution_count":null,"outputs":[{"data":{"text/plain":["{'Biology': 100.0,\n"," 'Mathematics': 100.0,\n"," 'PoliticalScience': 90.0,\n"," 'Psychology': 100.0,\n"," 'Statistics': 100.0}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"zynlFB-sspz8"},"source":["Looks like Political science is that hardest to classify, though this word has more syllables than the others and 30ms might not be a large enough time partition to capture this. "]},{"cell_type":"code","metadata":{"id":"MoNQFfmyspz9"},"source":[""],"execution_count":null,"outputs":[]}]}